# Crawler - Dieses Repository enthält einen einfachen in Python geschriebenen Webcrawler. Dieser dient nur als Beispiel für die millionenfache Nutzung in Google oder in Kartendiensten für die Suche nach Schlagwörtern.
Da der Webcrawler eine Link-Liste enthalten soll, muss diese integriert werden. Eine zwingend erforderliche Anforderung lautet, dass dieser im übertragenen Sinn „höflich“ sein muss, damit die gleichen Server nicht zu häufig abgefragt werden. Zudem müssen Informationen zur Latenz der robots.txt berücksichtigt werden. Zusätzlich zum crawlenden Link muss ein Zeitstempel erstellt werden, um ermitteln zu können, zu welchem Zeitpunkt eine bestimmte URL erneut aufgesucht werden darf. Für die Sortierung dieser Zeitstempel pro Domäne lässt sich ein Heap einset-zen (in Python heapq). Heapq arbeitet mit nativen Python-Listen. Die Menge der künftig zu durchsuchenden URLs wird auch Frontier genannt, daher wird die Klasse danach benannt. Der in diesem Repository enthaltene Python-Code enthält diverse Verfahren für das Laden und Auslesen von URLs. Zudem wird ein Verfahren ad-diert, um zu markieren, welche URL soeben aufgesucht wurde. Die Datenstrukturen werden in der Klasse frontiers inkludiert. Dazu zählt ein Dictionary, das nach Domäne anzeigt, welche URLs noch zu durchsuchen sind (urlparse). Zudem besteht ein URL-Set, das anzeigt, ob eine URL bereits be-sucht wurde sowie ein Heap (heapq), der die nächste zu durchsuchende Domäne abbildet. 

